Phase 1: Foundational Model Initialization
The process originates with a General Purpose Pre-trained SLM. This model serves as the baseline, equipped with broad capabilities but lacking the specific expertise required for technical applications.
Phase 2: Preference Alignment via Direct Preference Optimization (DPO)
The foundational model is first refined using DPO. This stage aligns the model's behavior with human-defined preferences, enhancing the relevance, correctness, and style of its generated outputs without the need for complex reward modeling.
Phase 3: Advanced Task-Specific Fine-Tuning
Following initial alignment, the model undergoes further specialization through one of two distinct pathways to master complex, domain-specific tasks:
Instruction-Tuned SLM: This path fine-tunes the model on curated datasets of instructions and desired responses, explicitly teaching it to follow complex commands and generate precise, structured outputs.
Reinforcement Fine-Tuned SLM (GRPO): This path utilizes Group Relative Policy Optimization, a reinforcement learning technique. The model's policy is optimized directly against a reward function to improve performance on nuanced tasks that benefit from iterative, reward-guided learning.
Phase 4: Model Compression and Efficiency Enhancement
The final, optional stage involves Width and Depth Pruning. This structural optimization technique systematically removes less critical parameters (i.e., neurons and entire layers) from the fine-tuned model.
The primary objective is to reduce the model's size and computational footprint, leading to improved inference latency and deployment efficiency with minimal degradation of task performance.
