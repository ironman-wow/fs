"Title: Automated generation of Chemical Process Engineering Schematics in Industrial Process Design.

We Claim:

1. A processor-implemented method (300), comprising:

receiving (302), via one or more hardware processors, a query specific to at least one of a process flow diagram and a piping and instrumentation diagram, from a user;

passing (304), via the one or more hardware processors, the received user query through one or more guardrails associated with one or more Large Language Models (LLMs) to filter the query;

retrieving (306), via the one or more hardware processors, context from a Memory database and a Graph database, based on the filtered query, using one or more fine-tuned Small Language Models (SLMs), wherein the one or more SLMs are fine-tuned by a process comprising the steps of:

(i) performing an Initial Direct Preference Optimization (DPO) alignment by training the one or more SLMs using a Direct Preference Optimization algorithm on a curated DPO dataset, the DPO dataset comprising chemical process engineering-related prompts paired with a ‚Äòchosen‚Äô preferred response and a rejected response; and

(ii) further fine-tuning the one or more SLMs, subsequent to the Initial DPO alignment, using at least one of:

(a) performing instruction tuning using one or more specialized synthetic datasets to enable the one or more SLMs to learn context-grounded generation; or

(b) performing Group Relative Policy Optimization (GRPO) reinforcement learning wherein:

(1) the one or more SLMs generate a group of candidate responses for a given chemical engineering task;
(2) the group of candidate responses are evaluated using a composite reward function that combines a plurality of metrics relevant to a quality of the at least one of a process flow diagram and a piping and instrumentation diagram generation; and
(3) one or more parameters of the one or more SLMs are updated to increase a probability of generating one or more responses that receive higher composite rewards relative to other responses within the group of candidate responses;

obtaining (308), via the one or more hardware processors, an initial response based on an inference from the retrieved context using the one or more fine-tuned SLMs;

optimizing (310), via the one or more hardware processors, the initial response using one or more inference optimization techniques;

performing iteratively (312), by a Critique Agent:

evaluating the initial response by utilizing one or more feedback mechanisms, the one or more feedback mechanisms including an SLM-as-Judge and a Reward Model-as-a-Judge, wherein the SLM-as-Judge is prompted with the initial response and one or more specific criteria for judgment;

providing, by the SLM-as-Judge, a critique including an identification of one or more potential errors, one or more inconsistencies, or one or more areas for improvement in the initial response; and

predicting, by the Reward Model-as-a-Judge, a scalar score reflecting a quality of the initial response, wherein a high score from the Reward Model-as-a-Judge indicates the initial response aligns well with one or more learned quality criteria, and a low score signals a need for refinement;

refining, via the one or more hardware processors, the initial response based on the critique from the SLM-as-a-Judge and the scalar score from the Reward Model-as-a-Judge; and

generating (314), via the one or more hardware processors, a final response to the user query based on the refined initial response.

2. The processor-implemented method as claimed in claim 1, wherein the filtering of the query identifies an irrelevant user query, and wherein the irrelevant user query comprises at least one of a security threat, a privacy invasion, an IP infringement, a hate speech, or a code interpreter abuse.

3. The processor-implemented method as claimed in claim 1, wherein the one or more SLMs are integrated with a Knowledge Graph (KG) using a Graph Retrieval-Augmented Generation (Graph RAG) framework to provide additional context when generating the initial response.

4. The processor-implemented method as claimed in claim 1, wherein the one or more inference optimization techniques for optimizing the initial response include at least one of structural pruning, FlashAttention, Paged KV Caching, or Lookahead Decoding."

Summary of Key Corrections and Reasons:

Claim 1 - Introduction:

"query specific to a process flow diagram and a piping and instrumentation diagram" changed to "query specific to at least one of a process flow diagram and a piping and instrumentation diagram" for broader scope and clarity (a query might be for one or both).

"guardrails of one or more Large Language Models (LLMs) to filter relevant query" changed to "one or more guardrails associated with one or more Large Language Models (LLMs) to filter the query" ‚Äì "associated with" is more accurate, and "filter the query" is more direct than "filter relevant query."

Claim 1 - SLM Fine-tuning Sub-steps:

Reworded for clarity and proper antecedent basis. "the one or more Small Language Models (SLMs)" changed to "the one or more SLMs" after first full mention.

(i) "paired with a ‚Äòchosen‚Äô (preferred) and a rejected response" - added "a" for clarity. "DPO dataset of chemical process..." changed to "DPO dataset comprising chemical process..."

(ii) "fine-tuning the Initial Direct Preference Optimization (DPO) alignment performed on one or more Small Language Models (SLMs)" changed to "further fine-tuning the one or more SLMs, subsequent to the Initial DPO alignment" for better flow and to clearly indicate sequence.

(a) "enable the one or more pre-trained Small Language Models (SLMs)" changed to "enable the one or more SLMs" because at this stage they are already being fine-tuned, not just pre-trained.

(b) Renumbered sub-sub-steps from (a), (b) to (1), (2), (3) for clarity within a sub-step.

(b)(1) "the one or more pre-trained Small Language Models (SLMs)" changed to "the one or more SLMs".

(b)(2) "plurality of metrics relevant to the process flow diagram and the piping and instrumentation diagram generation quality" changed to "plurality of metrics relevant to a quality of the at least one of a process flow diagram and a piping and instrumentation diagram generation" for consistency and accuracy.

(b)(3) "updating one or more parameters of the one or more pre-trained Small Language Models (SLMs)" changed to "one or more parameters of the one or more SLMs are updated". "probability of generating one or more responses which receive" changed to "that receive".

Claim 1 - Obtaining Inference/Initial Response:

"obtaining (308), via the one or more hardware processors, inference from the retrieved context using one or more fine-tuned Small Language Models (SLMs);" changed to "obtaining (308), via the one or more hardware processors, an initial response based on an inference from the retrieved context using the one or more fine-tuned SLMs;" ‚Äì "initial response" is the term used later and is clearer than just "inference."

Claim 1 - Optimizing Inference:

"optimizing (310), via the one or more hardware processors, the obtained inference using one or more optimization techniques;" changed to "optimizing (310), via the one or more hardware processors, the initial response using one or more inference optimization techniques;" ‚Äì "initial response" for consistency, and specified "inference optimization techniques" for clarity.

Claim 1 - Iterative Critique Agent Steps:

"till the ùëõ‚Äô" removed as it's vague without defining 'n'. "Iteratively" already implies repetition.

"evaluating the generated initial response or inference" changed to "evaluating the initial response" for consistency.

"one or more feedback mechanisms which includes" changed to "one or more feedback mechanisms, the one or more feedback mechanisms including".

"wherein the SLM is prompted with generated initial response" changed to "wherein the SLM-as-Judge is prompted with the initial response".

"providing a critique including identification of potential errors" changed to "providing, by the SLM-as-Judge, a critique including an identification of one or more potential errors, one or more inconsistencies, or one or more areas for improvement in the initial response".

"predicting a scalar score reflecting the quality of the SLM generated response" changed to "predicting, by the Reward Model-as-a-Judge, a scalar score reflecting a quality of the initial response".

"high score from the reward model indicates" changed to "a high score from the Reward Model-as-a-Judge indicates".

"the response aligns well with the learned quality criteria" changed to "the initial response aligns well with one or more learned quality criteria".

Claim 1 - Refining and Generating Final Response:

"refining (312), via the one or more hardware processors, the feedback from the SLM-as-a-Judge and Reward Model-as-a-Judge;" changed to "refining, via the one or more hardware processors, the initial response based on the critique from the SLM-as-a-Judge and the scalar score from the Reward Model-as-a-Judge;" to clarify what is being refined and based on what.

"generating (314), via the one or more hardware processors, a final response to the user query." changed to "generating (314), via the one or more hardware processors, a final response to the user query based on the refined initial response." to link the final response to the refinement.

Claim 2:

"wherein the irrelevant user query comprises at least one of a security threat, a privacy invasion, an IP infringement, a hate speech, and a code interpreter abuse." changed to "wherein the filtering of the query identifies an irrelevant user query, and wherein the irrelevant user query comprises at least one of a security threat, a privacy invasion, an IP infringement, a hate speech, or a code interpreter abuse." for better structure and "or" for lists.

Claim 3:

"while generating the final response" changed to "while generating the initial response" because the RAG context is used for the initial generation by the SLM, which is then refined.

Claim 4:

"wherein the initial response generated by the one or more SLMs is accelerated using one or more inference optimization techniques including a structural pruning, a FlashAttention, a Paged KV Caching, and a Lookahead Decoding." changed to "wherein the one or more inference optimization techniques for optimizing the initial response include at least one of structural pruning, FlashAttention, Paged KV Caching, or Lookahead Decoding." ‚Äì This clarifies it's about the optimization techniques and uses "at least one of" and "or" for proper claim language.

These changes aim to strengthen the claims by improving precision, clarity, and adherence to common patent drafting practices.
